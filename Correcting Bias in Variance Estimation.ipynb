{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f09e8bc",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Bessel's Correction : Biased & Unbiased Variance</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba46c80",
   "metadata": {},
   "source": [
    "* <u>**Author**</u> **:** [Younes Dahami](https://www.linkedin.com/in/dahami/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1a624",
   "metadata": {},
   "source": [
    "\n",
    "## Bessel's Correction : Adjusting Biased Sample Variance\n",
    "\n",
    "When calculating the variance of a population with size $N$, the formula is straightforward:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{\\sum (x - \\mu)^2}{N}\n",
    "$$\n",
    "\n",
    "Here, the summation is over all members $x$ of the population, and $\\mu$ is the population mean.\n",
    "\n",
    "However, when estimating $\\sigma^2$ using a sample, simply replacing $N$ with the sample size $n$, replacing $\\mu$ with the sample mean $\\bar{x}$, and summing over all members $x$ of the sample yields a **biased estimate** of $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "s^2_{\\text{biased}} = \\frac{\\sum (x - \\bar{x})^2}{n}\n",
    "$$\n",
    "\n",
    "To understand this bias intuitively, consider the extreme case where the sample size is $n=1$, with the lone value in the sample being $x_0$.\n",
    "In this case, $\\bar{x} = x_0$, making $s_{\\text{biased}}^2 = 0$. Estimating the population variance with $0$ is clearly an underestimate unless the population consisted of $N$ identical values.\n",
    "\n",
    "In the more general case, note that the sample mean is not the same as the population mean. One's sample observations are naturally going to be closer on average to the sample mean than the population mean, resulting in the average $(x - \\bar{x})^2$ value underestimating the average $(x - \\mu)^2$ value. Thus, $s^2_{\\text{biased}}$ generally underestimates $\\sigma^2$, with the difference more pronounced when the sample size is small.\n",
    "\n",
    "Check out the awesome YT channel [Statquest](https://youtu.be/sHRBg6BhKjI) video about the subject.\n",
    " \n",
    "The encouraging news is that we can correct this bias !\n",
    "\n",
    "Before delving into the argument, let's make a couple of observations:\n",
    "\n",
    "First, suppose that we randomly draw a sample of the form $\\{x_1, x_2, ..., x_n\\}$ from a population with mean $\\mu$. We can quickly prove that $E[\\bar{x}] = \\mu$, using the properties of the expected value, we can start by calculating the expected value of the sample mean, $\\bar{x}$ :\n",
    "\n",
    "$$\n",
    "E[\\bar{x}] = E\\left[\\frac{1}{n} \\sum_{i=1}^{n} x_i\\right]\n",
    "$$\n",
    "\n",
    "Since the expected value is a linear operator, we can move it inside the summation:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n} \\sum_{i=1}^{n} E[x_i]\n",
    "$$\n",
    "\n",
    "Given that each $x_i$ is drawn from a population with mean $\\mu$, we have:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n} \\sum_{i=1}^{n} \\mu\n",
    "$$\n",
    "\n",
    "Since $\\mu$ is constant with respect to $i$, we can simplify the sum:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n} \\cdot n \\cdot \\mu = \\mu\n",
    "$$\n",
    "\n",
    "Therefore, $E[\\bar{x}] = \\mu$.\n",
    "\n",
    "\n",
    "Second, assuming that the population discussed above has variance $\\sigma^2$, we can similarly demonstrate that $\\text{Var}[\\bar{x}] = \\sigma^2/n$, we start by calculating the variance of the sample mean, $\\bar{x}$:\n",
    "\n",
    "$$\n",
    "\\text{Var}[\\bar{x}] = \\text{Var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} x_i\\right]\n",
    "$$\n",
    "\n",
    "Since the random variables $x_i$ are assumed to be independent and identically distributed (i.i.d.), and the variance of a sum of independent random variables is the sum of their variances, we have:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}[x_i]\n",
    "$$\n",
    "\n",
    "Given that each $x_i$ is drawn from a population with variance $\\sigma^2$, we have:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n^2} \\cdot n \\cdot \\sigma^2 = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "Therefore, $\\text{Var}[\\bar{x}] = \\frac{\\sigma^2}{n}$.\n",
    "\n",
    "\n",
    "With these observations, let's proceed to the main argument. We aim to demonstrate that :\n",
    "\n",
    "$$\n",
    "E[s^2_{\\text{biased}}] = \\left(\\frac{n-1}{n}\\right) \\sigma^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "With the right side not being simply $\\sigma^2$, this equation establishes the biased nature of $s^2_{\\text{biased}}$ while simultaneously providing a corrective factor.\n",
    "\n",
    "To support our claim, consider the following : \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E[s^2_{\\text{biased}}] &= E\\left[\\frac{1}{n} \\cdot \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] \\\\\n",
    "&= E\\left[\\frac{1}{n} \\sum_{i=1}^{n} \\left[(x_i - \\mu) - (\\bar{x} - \\mu)\\right]^2\\right] \\\\\n",
    "&= E\\left[\\frac{1}{n} \\sum_{i=1}^{n} \\left[(x_i - \\mu)^2 - 2(\\bar{x} - \\mu)(x_i - \\mu) + (\\bar{x} - \\mu)^2\\right]\\right] \\\\\n",
    "&= E\\left[\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2 - 2(\\bar{x} - \\mu) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu) + \\frac{1}{n} \\sum_{i=1}^{n} (\\bar{x} - \\mu)^2\\right] \\\\\n",
    "&= E\\left[\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2 - 2(\\bar{x} - \\mu)^2 + \\frac{1}{n} \\cdot n \\cdot (\\bar{x} - \\mu)^2\\right] \\\\\n",
    "&= E\\left[\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2 - (\\bar{x} - \\mu)^2\\right] \\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^{n} E[(x_i - \\mu)^2] - E[(\\bar{x} - \\mu)^2] \\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^{n} \\sigma^2 - E[(\\bar{x} - \\mu)^2] \\\\\n",
    "&= \\frac{1}{n} \\cdot n \\cdot \\sigma^2 - E[(\\bar{x} - \\mu)^2] \\\\\n",
    "&= \\sigma^2 - E[(\\bar{x} - \\mu)^2] \\\\\n",
    "&= \\sigma^2 - \\text{Var}[\\bar{x}] \\\\\n",
    "&= \\sigma^2 - \\frac{\\sigma^2}{n} \\\\\n",
    "&= \\sigma^2 - \\frac{\\sigma^2}{n} \\\\\n",
    "&= \\frac{(n - 1)}{n} \\cdot \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Again, having established that $E[s^2_{\\text{biased}}] = \\left(\\frac{n-1}{n}\\right) \\sigma^2$, we can quickly construct an **unbiased estimator** $s^2$, for $\\sigma^2$ by multiplying $s^2_{\\text{biased}}$ by $n/(n-1)$:\n",
    "\n",
    "$$\n",
    "s^2 = \\frac{n}{n-1}.s^2_{\\text{biased}} = \\frac{n}{n-1}.\\frac{\\sum (x - \\bar{x})^2}{n} =\\frac{\\sum_{i=1}^n (x - \\bar{x})^2}{n - 1}\n",
    "$$\n",
    "\n",
    "The unbiased nature of $s^2$ can be promptly confirmed by observing :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E[s^2] &= E\\left[\\frac{1}{n-1} \\sum_{i=1}^{n}(x - \\bar{x})^2\\right] \\\\\n",
    "&= \\frac{1}{n-1} \\cdot E\\left[\\sum_{i=1}^{n}(x - \\bar{x})^2\\right] \\\\\n",
    "&= \\frac{n}{n-1} \\cdot E\\left[\\frac{1}{n} \\sum_{i=1}^{n}(x - \\bar{x})^2\\right] \\\\\n",
    "&= \\frac{n}{n-1} \\cdot E[s^2_{\\text{biased}}] \\\\\n",
    "&= \\frac{n}{n-1} \\cdot \\frac{n - 1}{n} \\cdot \\sigma^2 \\\\\n",
    "&= \\sigma^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ba95c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
